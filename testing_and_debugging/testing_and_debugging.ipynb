{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "406ec7b8",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe3795-674c-41f0-b31b-f4682be27d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import datetime as dt\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "import clickhouse_driver\n",
    "import vertica_python\n",
    "from confluent_kafka import Consumer, Producer\n",
    "\n",
    "from string import Template\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, DoubleType, StringType, TimestampType\n",
    ")\n",
    "\n",
    "import boto3\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from typing import Generator\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import connect\n",
    "\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec361e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=os.path.expandvars('/mnt/c/Users/igor_/.env'))\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('kafka')\n",
    "logger.setLevel(logging.WARN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a9bda57",
   "metadata": {},
   "source": [
    "## Бакет в S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID = os.environ['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "BUCKET_NAME = 'final-project'\n",
    "ENDPOINT_URL = 'https://storage.yandexcloud.net'\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3_resourse = session.resource(\n",
    "    service_name='s3',\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    ")\n",
    "\n",
    "bucket = s3_resourse.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb98202",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in s3_resourse.meta.client.list_objects_v2(Bucket='final-project')['Contents']:\n",
    "    print(file['Key'], file['LastModified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in bucket.objects.all():\n",
    "    buffer = io.BytesIO()\n",
    "    bucket.download_fileobj(obj.key, buffer)\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    df = pd.read_csv(buffer)\n",
    "    print(df.shape)\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2472471",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = io.BytesIO()\n",
    "bucket.download_fileobj('currencies_history.csv', buffer)\n",
    "buffer.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4baa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = buffer.readline().decode(\"utf-8\").rstrip()\n",
    "buffer.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.getvalue()\n",
    "pd.read_csv(buffer).to_dict('split')['data']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f0f2165",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME']='/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "\n",
    "# import findspark\n",
    "# findspark.add_packages('com.clickhouse:clickhouse-jdbc:0.4.6')\n",
    "# findspark.add_jars('/home/ishirokov/jars/clickhouse-jdbc-0.4.6-all.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7487c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkConnector():\n",
    "    def __init__(self, spark_jars_packages,\n",
    "                 app_name=f\"app_{dt.datetime.now().strftime('%Y%m%d')}\") -> None:\n",
    "        self.spark = (\n",
    "            SparkSession\n",
    "                .builder\n",
    "                .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "                .config(\"spark.jars.packages\", spark_jars_packages)\n",
    "                .appName(app_name)\n",
    "                .getOrCreate()\n",
    "        )\n",
    "        self.spark.sparkContext.setLogLevel('ERROR')\n",
    "    \n",
    "    def load_df(self, starting_offset: int, ending_offset: int, kafka_options: dict) -> DataFrame:\n",
    "        offsets = Template(\"\"\"{\"transaction-service-input\":{\"0\":$offset,\"1\":$offset}}\"\"\")\n",
    "        \n",
    "        return (\n",
    "            self.spark.read\n",
    "            .format(\"kafka\")\n",
    "            .options(**kafka_options)\n",
    "            .option(\"startingOffsets\", offsets.substitute(offset=starting_offset))\n",
    "            .option(\"endingOffsets\", offsets.substitute(offset=ending_offset))\n",
    "            .option(\"failOnDataLoss\", \"false\")\n",
    "            .load()\n",
    "        )\n",
    "    \n",
    "    def read_stream(self, kafka_options: dict):      \n",
    "        return (\n",
    "            self.spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .options(**kafka_options)\n",
    "            .option(\"startingOffsets\", \"latest\")\n",
    "            .option(\"failOnDataLoss\", \"false\")\n",
    "            .load()\n",
    "        )\n",
    "    \n",
    "    def transform_df(self, df: DataFrame, field: str, schema,\n",
    "                     **kwargs) -> DataFrame:\n",
    "        df = df.withColumn('value',  F.col(field).cast('string')) \\\n",
    "            .withColumn('data', F.from_json(F.col('value'), schema=schema)) \\\n",
    "            .selectExpr('data.*')\n",
    "        if kwargs:\n",
    "            return df.dropDuplicates([kwargs['id']]) \\\n",
    "                .withWatermark(kwargs['watermark'], kwargs['interval'])\n",
    "        else:\n",
    "            return df\n",
    "        \n",
    "    def read_pg_table(self, table: str, spark: SparkSession,\n",
    "                      pg_settings) -> DataFrame:\n",
    "        df = (\n",
    "            self.spark.read\n",
    "                .format(\"jdbc\")\n",
    "                .option(\"driver\", \"org.postgresql.Driver\")\n",
    "                .option(\"dbtable\", table)\n",
    "                .options(**pg_settings)\n",
    "                .load()\n",
    "        )\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_jars_packages = \",\".join(\n",
    "    [\n",
    "        \"org.postgresql:postgresql:42.4.0\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "kafka_pass = Template('org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"producer_consumer\\\" password=\\\"$kafka_pass\\\";')\n",
    "\n",
    "kafka_options = {\n",
    "    'subscribe': 'transaction-service-input',\n",
    "    'kafka.bootstrap.servers': 'rc1a-083hhcof7uqe71hd.mdb.yandexcloud.net:9091',\n",
    "    'kafka.security.protocol': 'SASL_SSL',\n",
    "    'kafka.sasl.mechanism': 'SCRAM-SHA-512',\n",
    "    'kafka.sasl.jaas.config': kafka_pass.substitute(kafka_pass=os.environ['KAFKA_PASSWORD']),\n",
    "    'kafka.ssl.truststore.location': '/mnt/c/ca/YandexInternalRootCA.jks',\n",
    "    'kafka.ssl.truststore.password': os.environ['TRUSTSTORE_PASSWORD']\n",
    "}\n",
    "\n",
    "spark = SparkConnector(spark_jars_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad69335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read_stream(kafka_options)\n",
    "\n",
    "q = (\n",
    "    df.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"query_0\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(q.status)\n",
    "    display(spark.spark.sql('SELECT * FROM query_0').show())\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc4370",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_schema = StructType([\n",
    "    StructField(\"object_id\", StringType(), nullable=True),\n",
    "    StructField(\"object_type\", StringType(), nullable=True),\n",
    "    StructField(\"sent_dttm\", TimestampType(), nullable=True),\n",
    "    StructField(\"payload\", StringType(), nullable=True),\n",
    "])\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"operation_id\", StringType(), nullable=True),\n",
    "    StructField(\"account_number_from\", IntegerType(), nullable=True),\n",
    "    StructField(\"account_number_to\", IntegerType(), nullable=True),\n",
    "    StructField(\"currency_code\", IntegerType(), nullable=True),\n",
    "    StructField(\"country\", StringType(), nullable=True),\n",
    "    StructField(\"status\", StringType(), nullable=True),\n",
    "    StructField(\"transaction_type\", StringType(), nullable=True),\n",
    "    StructField(\"amount\", IntegerType(), nullable=True),\n",
    "    StructField(\"transaction_dt\", TimestampType(), nullable=True),\n",
    "])\n",
    "\n",
    "df = spark.load_df(0, -1, kafka_options)\n",
    "\n",
    "kwargs = {'id': 'object_id', 'watermark': 'sent_dttm', 'interval': '60 minutes'}\n",
    "tr_df = spark.transform_df(df, 'value', msg_schema, **kwargs)\n",
    "\n",
    "tr_df = tr_df.filter(F.col('object_type') == F.lit('TRANSACTION'))\n",
    "transactions = spark.transform_df(tr_df, 'payload', transaction_schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21963951",
   "metadata": {},
   "source": [
    "## PG to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PgConnect:\n",
    "    def __init__(self, host: str, port: int, db_name: str, user: str, pw: str, sslmode: str = \"require\") -> None:\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.db_name = db_name\n",
    "        self.user = user\n",
    "        self.pw = pw\n",
    "        self.sslmode = sslmode\n",
    "\n",
    "    def url(self) -> str:\n",
    "        return \"\"\"\n",
    "            host={host}\n",
    "            port={port}\n",
    "            dbname={db_name}\n",
    "            user={user}\n",
    "            password={pw}\n",
    "            target_session_attrs=read-write\n",
    "            sslmode={sslmode}\n",
    "        \"\"\".format(\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            db_name=self.db_name,\n",
    "            user=self.user,\n",
    "            pw=self.pw,\n",
    "            sslmode=self.sslmode)\n",
    "\n",
    "    @contextmanager\n",
    "    def connection(self) -> Generator[connect, None, None]:\n",
    "        conn = psycopg2.connect(self.url())\n",
    "        try:\n",
    "            yield conn\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            raise e\n",
    "        finally:\n",
    "            conn.close()\n",
    "            \n",
    "            \n",
    "db = PgConnect(\n",
    "    host=\"rc1b-w5d285tmxa8jimyn.mdb.yandexcloud.net\", port=6432,\n",
    "    db_name=\"db1\", user=os.environ['PG_USER'], pw=os.environ['PG_PASSWORD'], sslmode=\"require\"\n",
    ")\n",
    "\n",
    "with db.connection() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT operation_id, account_number_from, account_number_to, currency_code, country, status, transaction_type, amount, transaction_dt\n",
    "            FROM public.transactions;\n",
    "\n",
    "            \"\"\",\n",
    "        )\n",
    "        conn.commit()\n",
    "        res = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f824cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_callback(err):\n",
    "    print('Error message: {}'.format(err))\n",
    "\n",
    "\n",
    "class KafkaProducer:\n",
    "    def __init__(self, host: str, port: int, user: str, password: str, topic: str, cert_path: str) -> None:\n",
    "        params = {\n",
    "            'bootstrap.servers': f'{host}:{port}',\n",
    "            'security.protocol': 'SASL_SSL',\n",
    "            'ssl.ca.location': cert_path,\n",
    "            'sasl.mechanism': 'SCRAM-SHA-512',\n",
    "            'sasl.username': user,\n",
    "            'sasl.password': password,\n",
    "            'error_cb': error_callback,\n",
    "        }\n",
    "\n",
    "        self.topic = topic\n",
    "        self.p = Producer(params)\n",
    "\n",
    "    def produce(self, payload) -> None:\n",
    "        self.p.produce(self.topic, json.dumps(payload))\n",
    "        self.p.flush(10)\n",
    "\n",
    "class KafkaProducer:\n",
    "    def __init__(self, host: str, port: int, user: str, password: str, topic: str, cert_path: str) -> None:\n",
    "        params = {\n",
    "            'bootstrap.servers': f'{host}:{port}',\n",
    "            'security.protocol': 'SASL_SSL',\n",
    "            'ssl.ca.location': cert_path,\n",
    "            'sasl.mechanism': 'SCRAM-SHA-512',\n",
    "            'sasl.username': user,\n",
    "            'sasl.password': password,\n",
    "            'error_cb': error_callback,\n",
    "        }\n",
    "\n",
    "        self.topic = topic\n",
    "        self.p = Producer(params)\n",
    "\n",
    "    def produce(self, payload) -> None:\n",
    "        self.p.produce(self.topic, json.dumps(payload))\n",
    "        self.p.flush(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_prod = KafkaProducer(\n",
    "    'rc1a-083hhcof7uqe71hd.mdb.yandexcloud.net',\n",
    "    9091,\n",
    "    'producer_consumer',\n",
    "    os.environ['KAFKA_PASSWORD'],\n",
    "    'transaction-service-input',\n",
    "    '/mnt/c/ca/YandexInternalRootCA.crt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'operation_id',\n",
    "    'account_number_from',\n",
    "    'account_number_to',\n",
    "    'currency_code',\n",
    "    'country',\n",
    "    'status',\n",
    "    'transaction_type',\n",
    "    'amount',\n",
    "    'transaction_dt'\n",
    "]\n",
    "\n",
    "for tp in df.itertuples():\n",
    "    k_prod.produce(\n",
    "        dict(\n",
    "            operation_id=tp.operation_id,\n",
    "            account_number_from=tp.account_number_from,\n",
    "            account_number_to=tp.account_number_to,\n",
    "            currency_code=tp.currency_code,\n",
    "            country=tp.country,\n",
    "            status=tp.status,\n",
    "            transaction_type=tp.transaction_type,\n",
    "            amount=tp.amount,\n",
    "            transaction_dt=tp.transaction_dt,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66f146b5",
   "metadata": {},
   "source": [
    "## Vertica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_info = {\n",
    "    'host': '51.250.75.20',\n",
    "    'port': 5433,\n",
    "    'user': os.environ['VERTICA_USER'],\n",
    "    'password': os.environ['VERTICA_PASSWORD'],\n",
    "    'database': 'dwh',\n",
    "    'autocommit': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119457a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with vertica_python.connect(**conn_info) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT DISTINCT table_name FROM all_tables WHERE table_name LIKE 'tran%\")\n",
    "        print(cur.fetchall())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6db9077",
   "metadata": {},
   "source": [
    "## Clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_info = dict(\n",
    "    host='rc1a-pcouajriqbdi97o4.mdb.yandexcloud.net',\n",
    "    port=9440,\n",
    "    database='dwh',\n",
    "    user=os.environ['CH_USER'],\n",
    "    password=os.environ['CH_PASSWORD'],\n",
    "    secure=True,\n",
    "    ca_certs='/mnt/c/ca/YandexInternalRootCA.crt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with clickhouse_driver.connect(**conn_info) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute('SHOW TABLES')\n",
    "        print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb379e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
